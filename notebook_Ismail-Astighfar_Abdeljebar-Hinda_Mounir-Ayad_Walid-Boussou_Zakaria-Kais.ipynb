{"cells":[{"cell_type":"markdown","metadata":{},"source":["## In this notebook, we will Fine Tune a **RoBERTa** Transformer for Sentiment Analysis\n","\n"," - HINDA Abdeljebar\n"," - BOUSSOU Walid\n"," - ASTIGHFAR Ismail\n"," - KAIS Zakaria\n"," - AYAD Mounir\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Importing Python Libraries and Setting Up the Environment\n","\n","In this step, we will import the necessary libraries and modules to execute our script. The libraries include:\n","* Pandas\n","* PyTorch\n","* PyTorch Utils for Dataset and Dataloader\n","* Transformers\n","* RoBERTa Model and Tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:28.059309Z","iopub.status.busy":"2024-02-10T10:58:28.058567Z","iopub.status.idle":"2024-02-10T10:58:35.599956Z","shell.execute_reply":"2024-02-10T10:58:35.599170Z","shell.execute_reply.started":"2024-02-10T10:58:28.059279Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import transformers\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaModel, RobertaTokenizer\n","\n","import logging\n","logging.basicConfig(level=logging.ERROR)"]},{"cell_type":"markdown","metadata":{},"source":["#### Configuring the Device for GPU Usage:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.602047Z","iopub.status.busy":"2024-02-10T10:58:35.601609Z","iopub.status.idle":"2024-02-10T10:58:35.625291Z","shell.execute_reply":"2024-02-10T10:58:35.624356Z","shell.execute_reply.started":"2024-02-10T10:58:35.602021Z"},"trusted":true},"outputs":[],"source":["\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["#### Loading Training Data from the 'train.tsv' File:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.630947Z","iopub.status.busy":"2024-02-10T10:58:35.630604Z","iopub.status.idle":"2024-02-10T10:58:35.883397Z","shell.execute_reply":"2024-02-10T10:58:35.882397Z","shell.execute_reply.started":"2024-02-10T10:58:35.630916Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', delimiter='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.884847Z","iopub.status.busy":"2024-02-10T10:58:35.884560Z","iopub.status.idle":"2024-02-10T10:58:35.891622Z","shell.execute_reply":"2024-02-10T10:58:35.890708Z","shell.execute_reply.started":"2024-02-10T10:58:35.884824Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(156060, 4)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.893005Z","iopub.status.busy":"2024-02-10T10:58:35.892728Z","iopub.status.idle":"2024-02-10T10:58:35.909322Z","shell.execute_reply":"2024-02-10T10:58:35.908463Z","shell.execute_reply.started":"2024-02-10T10:58:35.892983Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PhraseId  SentenceId                                             Phrase  \\\n","0         1           1  A series of escapades demonstrating the adage ...   \n","1         2           1  A series of escapades demonstrating the adage ...   \n","2         3           1                                           A series   \n","3         4           1                                                  A   \n","4         5           1                                             series   \n","\n","   Sentiment  \n","0          1  \n","1          2  \n","2          2  \n","3          2  \n","4          2  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.910758Z","iopub.status.busy":"2024-02-10T10:58:35.910422Z","iopub.status.idle":"2024-02-10T10:58:35.920419Z","shell.execute_reply":"2024-02-10T10:58:35.919469Z","shell.execute_reply.started":"2024-02-10T10:58:35.910728Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([1, 2, 3, 4, 0])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train['Sentiment'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.921684Z","iopub.status.busy":"2024-02-10T10:58:35.921449Z","iopub.status.idle":"2024-02-10T10:58:35.952955Z","shell.execute_reply":"2024-02-10T10:58:35.952121Z","shell.execute_reply.started":"2024-02-10T10:58:35.921664Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>78030.500000</td>\n","      <td>4079.732744</td>\n","      <td>2.063578</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>45050.785842</td>\n","      <td>2502.764394</td>\n","      <td>0.893832</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>39015.750000</td>\n","      <td>1861.750000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>78030.500000</td>\n","      <td>4017.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>117045.250000</td>\n","      <td>6244.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>156060.000000</td>\n","      <td>8544.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            PhraseId     SentenceId      Sentiment\n","count  156060.000000  156060.000000  156060.000000\n","mean    78030.500000    4079.732744       2.063578\n","std     45050.785842    2502.764394       0.893832\n","min         1.000000       1.000000       0.000000\n","25%     39015.750000    1861.750000       2.000000\n","50%     78030.500000    4017.000000       2.000000\n","75%    117045.250000    6244.000000       3.000000\n","max    156060.000000    8544.000000       4.000000"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train.describe()"]},{"cell_type":"markdown","metadata":{},"source":["#### We will keep the two columns 'Phrase' (attribute) and 'Sentiment' (target):\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.954476Z","iopub.status.busy":"2024-02-10T10:58:35.954156Z","iopub.status.idle":"2024-02-10T10:58:35.972245Z","shell.execute_reply":"2024-02-10T10:58:35.971342Z","shell.execute_reply.started":"2024-02-10T10:58:35.954447Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>156055</th>\n","      <td>Hearst 's</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>156056</th>\n","      <td>forced avuncular chortles</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>156057</th>\n","      <td>avuncular chortles</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>156058</th>\n","      <td>avuncular</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>156059</th>\n","      <td>chortles</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>156060 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   Phrase  Sentiment\n","0       A series of escapades demonstrating the adage ...          1\n","1       A series of escapades demonstrating the adage ...          2\n","2                                                A series          2\n","3                                                       A          2\n","4                                                  series          2\n","...                                                   ...        ...\n","156055                                          Hearst 's          2\n","156056                          forced avuncular chortles          1\n","156057                                 avuncular chortles          3\n","156058                                          avuncular          2\n","156059                                           chortles          2\n","\n","[156060 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["new_df = train[['Phrase', 'Sentiment']]\n","new_df"]},{"cell_type":"markdown","metadata":{},"source":["#### Defining Key Variables to be Used Later in Training and Validadtion :\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:35.976598Z","iopub.status.busy":"2024-02-10T10:58:35.976089Z","iopub.status.idle":"2024-02-10T10:58:39.138898Z","shell.execute_reply":"2024-02-10T10:58:39.137899Z","shell.execute_reply.started":"2024-02-10T10:58:35.976575Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc8872a8627845cebb7b690e946cbbc5","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a47a715f9bf442eeae45e0433c4cd425","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7566e53351b14a3e8a81ba32e3407ac4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"833b7ad536b541b5a0535bbbb350ede4","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["MAX_LEN = 52\n","\n","TRAIN_BATCH_SIZE = 64\n","\n","VALID_BATCH_SIZE = 32\n","\n","LEARNING_RATE = 1e-5\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### Preparing the Dataset and Dataloader :\n","This class is defined to accept the Dataframe as input and generate tokenized output that is used by the Roberta model for training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:39.140528Z","iopub.status.busy":"2024-02-10T10:58:39.140171Z","iopub.status.idle":"2024-02-10T10:58:39.151201Z","shell.execute_reply":"2024-02-10T10:58:39.150195Z","shell.execute_reply.started":"2024-02-10T10:58:39.140497Z"},"trusted":true},"outputs":[],"source":["class SentimentData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.Phrase\n","        self.targets = self.data.Sentiment\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["#### Fraction of Data Used for Training and Validation :\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:39.152835Z","iopub.status.busy":"2024-02-10T10:58:39.152478Z","iopub.status.idle":"2024-02-10T10:58:39.210022Z","shell.execute_reply":"2024-02-10T10:58:39.209076Z","shell.execute_reply.started":"2024-02-10T10:58:39.152804Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FULL Dataset: (156060, 2)\n","TRAIN Dataset: (124848, 2)\n","TEST Dataset: (31212, 2)\n"]}],"source":["train_size = 0.8\n","train_data=new_df.sample(frac=train_size,random_state=200)\n","val_data=new_df.drop(train_data.index).reset_index(drop=True)\n","train_data = train_data.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(new_df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_data.shape))\n","print(\"VALIDATION Dataset: {}\".format(val_data.shape))\n","\n","training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n","validation_set = SentimentData(val_data, tokenizer, MAX_LEN)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Configuring Training and Validation Parameters with Creation of Corresponding Data Loaders\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:39.211571Z","iopub.status.busy":"2024-02-10T10:58:39.211216Z","iopub.status.idle":"2024-02-10T10:58:39.217796Z","shell.execute_reply":"2024-02-10T10:58:39.216751Z","shell.execute_reply.started":"2024-02-10T10:58:39.211539Z"},"trusted":true},"outputs":[],"source":["\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 4\n","                }\n","\n","val_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 4\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","validation_loader = DataLoader(validation_set, **val_params)"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Neural Network for Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:39.219270Z","iopub.status.busy":"2024-02-10T10:58:39.218974Z","iopub.status.idle":"2024-02-10T10:58:39.228902Z","shell.execute_reply":"2024-02-10T10:58:39.228005Z","shell.execute_reply.started":"2024-02-10T10:58:39.219246Z"},"trusted":true},"outputs":[],"source":["class RobertaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(RobertaClass, self).__init__()\n","        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","    \n","        self.dropout = torch.nn.Dropout(0.3)\n","     \n","        self.classifier = torch.nn.Linear(768, 5)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    \n","        hidden_state = output_1[0]\n","     \n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","       \n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:39.230266Z","iopub.status.busy":"2024-02-10T10:58:39.230007Z","iopub.status.idle":"2024-02-10T10:58:54.571385Z","shell.execute_reply":"2024-02-10T10:58:54.570401Z","shell.execute_reply.started":"2024-02-10T10:58:39.230244Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"197b2e512ac74cfa843d725aa2155c10","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["RobertaClass(\n","  (l1): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model = RobertaClass()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["#### Loss Function and Optimizer :\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:54.573724Z","iopub.status.busy":"2024-02-10T10:58:54.573077Z","iopub.status.idle":"2024-02-10T10:58:54.579864Z","shell.execute_reply":"2024-02-10T10:58:54.578937Z","shell.execute_reply.started":"2024-02-10T10:58:54.573667Z"},"trusted":true},"outputs":[],"source":["loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE,weight_decay=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:54.581253Z","iopub.status.busy":"2024-02-10T10:58:54.580993Z","iopub.status.idle":"2024-02-10T10:58:54.593252Z","shell.execute_reply":"2024-02-10T10:58:54.592562Z","shell.execute_reply.started":"2024-02-10T10:58:54.581226Z"},"trusted":true},"outputs":[],"source":["def calculate_accuracy(preds, targets):\n","    n_correct = (preds==targets).sum().item()\n","    return n_correct"]},{"cell_type":"markdown","metadata":{},"source":["#### Training Function for RoBERTa Sentiment Analysis Model\n","Here we define a training function that trains the model on the training dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:54.594787Z","iopub.status.busy":"2024-02-10T10:58:54.594254Z","iopub.status.idle":"2024-02-10T10:58:54.604545Z","shell.execute_reply":"2024-02-10T10:58:54.603656Z","shell.execute_reply.started":"2024-02-10T10:58:54.594764Z"},"trusted":true},"outputs":[],"source":["\n","\n","def train(epoch):\n","   \n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    \n","    model.train()\n","\n","    for _,data in tqdm(enumerate(training_loader, 0)):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.long)\n","\n","       \n","        outputs = model(ids, mask, token_type_ids)\n","\n","       \n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","\n","       \n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calculate_accuracy(big_idx, targets)\n","\n","     \n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","\n","       \n","        if _%5000==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            accu_step = (n_correct*100)/nb_tr_examples\n","            print(f\"Training Loss per 5000 steps: {loss_step}\")\n","            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n","\n","       \n","        optimizer.zero_grad()\n","        loss.backward()\n","      \n","        optimizer.step()\n","    \n","    \n","    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Training Loss Epoch: {epoch_loss}\")\n","    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n","\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T10:58:54.605958Z","iopub.status.busy":"2024-02-10T10:58:54.605681Z","iopub.status.idle":"2024-02-10T11:47:42.953025Z","shell.execute_reply":"2024-02-10T11:47:42.951940Z","shell.execute_reply.started":"2024-02-10T10:58:54.605936Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Training Loss per 5000 steps: 1.6614089012145996\n","Training Accuracy per 5000 steps: 15.625\n"]},{"name":"stderr","output_type":"stream","text":["1951it [09:45,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["The Total Accuracy for Epoch 0: 65.20569011918492\n","Training Loss Epoch: 0.8463560834656857\n","Training Accuracy Epoch: 65.20569011918492\n"]},{"name":"stderr","output_type":"stream","text":["\n","1it [00:00,  4.16it/s]"]},{"name":"stdout","output_type":"stream","text":["Training Loss per 5000 steps: 0.8710432052612305\n","Training Accuracy per 5000 steps: 59.375\n"]},{"name":"stderr","output_type":"stream","text":["1951it [09:45,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["The Total Accuracy for Epoch 1: 69.8024798154556\n","Training Loss Epoch: 0.7252132300839309\n","Training Accuracy Epoch: 69.8024798154556\n"]},{"name":"stderr","output_type":"stream","text":["\n","1it [00:00,  4.99it/s]"]},{"name":"stdout","output_type":"stream","text":["Training Loss per 5000 steps: 0.6515316963195801\n","Training Accuracy per 5000 steps: 73.4375\n"]},{"name":"stderr","output_type":"stream","text":["1951it [09:45,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["The Total Accuracy for Epoch 2: 71.69918621043189\n","Training Loss Epoch: 0.6770083618616215\n","Training Accuracy Epoch: 71.69918621043189\n"]},{"name":"stderr","output_type":"stream","text":["\n","1it [00:00,  4.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Training Loss per 5000 steps: 0.8445261716842651\n","Training Accuracy per 5000 steps: 57.8125\n"]},{"name":"stderr","output_type":"stream","text":["1951it [09:45,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["The Total Accuracy for Epoch 3: 73.39965397923875\n","Training Loss Epoch: 0.6399519480478452\n","Training Accuracy Epoch: 73.39965397923875\n"]},{"name":"stderr","output_type":"stream","text":["\n","1it [00:00,  4.75it/s]"]},{"name":"stdout","output_type":"stream","text":["Training Loss per 5000 steps: 0.5559527277946472\n","Training Accuracy per 5000 steps: 75.0\n"]},{"name":"stderr","output_type":"stream","text":["1951it [09:44,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["The Total Accuracy for Epoch 4: 75.03123798539023\n","Training Loss Epoch: 0.6044249981565393\n","Training Accuracy Epoch: 75.03123798539023\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    train(epoch)"]},{"cell_type":"markdown","metadata":{},"source":["#### Validation Function for RoBERTa Sentiment Analysis Model\n","During the validation stage we pass the unseen data(Validation Dataset) to the model. This step determines how good the model performs on the unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T11:47:42.955309Z","iopub.status.busy":"2024-02-10T11:47:42.955008Z","iopub.status.idle":"2024-02-10T11:47:42.966356Z","shell.execute_reply":"2024-02-10T11:47:42.965314Z","shell.execute_reply.started":"2024-02-10T11:47:42.955280Z"},"trusted":true},"outputs":[],"source":["def valid(model, validation_loader):\n","\n","  \n","    model.eval()\n","\n","    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n","\n","   \n","    with torch.no_grad():\n","        for _, data in tqdm(enumerate(validation_loader, 0)):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype = torch.long)\n","            outputs = model(ids, mask, token_type_ids).squeeze()\n","\n","           \n","            loss = loss_function(outputs, targets)\n","            tr_loss += loss.item()\n","            big_val, big_idx = torch.max(outputs.data, dim=1)\n","            n_correct += calculate_accuracy(big_idx, targets)\n","\n","            nb_tr_steps += 1\n","            nb_tr_examples+=targets.size(0)\n","\n","          \n","            if _%5000==0:\n","                loss_step = tr_loss/nb_tr_steps\n","                accu_step = (n_correct*100)/nb_tr_examples\n","                print(f\"Validation Loss per 100 steps: {loss_step}\")\n","                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n","                \n"," \n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Validation Loss Epoch: {epoch_loss}\")\n","    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n","\n","    return epoch_accu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-10T11:47:42.967955Z","iopub.status.busy":"2024-02-10T11:47:42.967641Z","iopub.status.idle":"2024-02-10T11:48:31.498236Z","shell.execute_reply":"2024-02-10T11:48:31.497157Z","shell.execute_reply.started":"2024-02-10T11:47:42.967931Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2it [00:00,  7.46it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss per 100 steps: 0.5872433185577393\n","Validation Accuracy per 100 steps: 75.0\n"]},{"name":"stderr","output_type":"stream","text":["488it [00:48, 10.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss Epoch: 0.733800831266114\n","Validation Accuracy Epoch: 70.23901063693451\n","Accuracy on test data = 70.24%\n"]}],"source":["acc = valid(model, validation_loader)\n","print(\"Accuracy on test data = %0.2f%%\" % acc)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":32092,"sourceId":10025,"sourceType":"competition"},{"datasetId":4511,"sourceId":6897,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
