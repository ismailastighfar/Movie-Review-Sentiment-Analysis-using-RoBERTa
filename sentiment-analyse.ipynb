{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10025,"databundleVersionId":32092,"sourceType":"competition"},{"sourceId":6897,"sourceType":"datasetVersion","datasetId":4511}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## In this notebook, we will Fine Tune a **RoBERTa** Transformer for Sentiment Analysis\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.229695Z","iopub.execute_input":"2024-02-10T14:35:59.230688Z","iopub.status.idle":"2024-02-10T14:35:59.236734Z","shell.execute_reply.started":"2024-02-10T14:35:59.230648Z","shell.execute_reply":"2024-02-10T14:35:59.235792Z"}}},{"cell_type":"markdown","source":"#### Importing Python Libraries and Setting Up the Environment\n\nIn this step, we will import the necessary libraries and modules to execute our script. The libraries include:\n* Pandas\n* PyTorch\n* PyTorch Utils for Dataset and Dataloader\n* Transformers\n* RoBERTa Model and Tokenizer\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\n\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:22.880346Z","iopub.execute_input":"2024-02-10T15:54:22.880715Z","iopub.status.idle":"2024-02-10T15:54:22.886733Z","shell.execute_reply.started":"2024-02-10T15:54:22.880679Z","shell.execute_reply":"2024-02-10T15:54:22.885690Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"#### Configuring the Device for GPU Usage:\n","metadata":{}},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:28.504556Z","iopub.execute_input":"2024-02-10T15:54:28.505050Z","iopub.status.idle":"2024-02-10T15:54:28.509610Z","shell.execute_reply.started":"2024-02-10T15:54:28.505015Z","shell.execute_reply":"2024-02-10T15:54:28.508461Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"#### Loading Training Data from the 'train.tsv' File:\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', delimiter='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:31.629886Z","iopub.execute_input":"2024-02-10T15:54:31.630748Z","iopub.status.idle":"2024-02-10T15:54:31.839177Z","shell.execute_reply.started":"2024-02-10T15:54:31.630705Z","shell.execute_reply":"2024-02-10T15:54:31.838327Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:33.768444Z","iopub.execute_input":"2024-02-10T15:54:33.769180Z","iopub.status.idle":"2024-02-10T15:54:33.775059Z","shell.execute_reply.started":"2024-02-10T15:54:33.769146Z","shell.execute_reply":"2024-02-10T15:54:33.774136Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"(156060, 4)"},"metadata":{}}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:36.328773Z","iopub.execute_input":"2024-02-10T15:54:36.329494Z","iopub.status.idle":"2024-02-10T15:54:36.340165Z","shell.execute_reply.started":"2024-02-10T15:54:36.329458Z","shell.execute_reply":"2024-02-10T15:54:36.339194Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"   PhraseId  SentenceId                                             Phrase  \\\n0         1           1  A series of escapades demonstrating the adage ...   \n1         2           1  A series of escapades demonstrating the adage ...   \n2         3           1                                           A series   \n3         4           1                                                  A   \n4         5           1                                             series   \n\n   Sentiment  \n0          1  \n1          2  \n2          2  \n3          2  \n4          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['Sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:40.742549Z","iopub.execute_input":"2024-02-10T15:54:40.743475Z","iopub.status.idle":"2024-02-10T15:54:40.750670Z","shell.execute_reply.started":"2024-02-10T15:54:40.743432Z","shell.execute_reply":"2024-02-10T15:54:40.749666Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"array([1, 2, 3, 4, 0])"},"metadata":{}}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:47.971351Z","iopub.execute_input":"2024-02-10T15:54:47.971697Z","iopub.status.idle":"2024-02-10T15:54:47.998113Z","shell.execute_reply.started":"2024-02-10T15:54:47.971669Z","shell.execute_reply":"2024-02-10T15:54:47.997186Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"            PhraseId     SentenceId      Sentiment\ncount  156060.000000  156060.000000  156060.000000\nmean    78030.500000    4079.732744       2.063578\nstd     45050.785842    2502.764394       0.893832\nmin         1.000000       1.000000       0.000000\n25%     39015.750000    1861.750000       2.000000\n50%     78030.500000    4017.000000       2.000000\n75%    117045.250000    6244.000000       3.000000\nmax    156060.000000    8544.000000       4.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>78030.500000</td>\n      <td>4079.732744</td>\n      <td>2.063578</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>45050.785842</td>\n      <td>2502.764394</td>\n      <td>0.893832</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>39015.750000</td>\n      <td>1861.750000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>78030.500000</td>\n      <td>4017.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>117045.250000</td>\n      <td>6244.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>156060.000000</td>\n      <td>8544.000000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### We will keep the two columns 'Phrase' (attribute) and 'Sentiment' (target):\n","metadata":{}},{"cell_type":"code","source":"new_df = train[['Phrase', 'Sentiment']]\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:57.102897Z","iopub.execute_input":"2024-02-10T15:54:57.103724Z","iopub.status.idle":"2024-02-10T15:54:57.123385Z","shell.execute_reply.started":"2024-02-10T15:54:57.103690Z","shell.execute_reply":"2024-02-10T15:54:57.122402Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"                                              Phrase  Sentiment\n0  A series of escapades demonstrating the adage ...          1\n1  A series of escapades demonstrating the adage ...          2\n2                                           A series          2\n3                                                  A          2\n4                                             series          2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Defining Key Variables to be Used Later in Training and Validadtion :\n","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 52\n\nTRAIN_BATCH_SIZE = 64\n\nVALID_BATCH_SIZE = 32\n\nLEARNING_RATE = 5e-05\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.547374Z","iopub.execute_input":"2024-02-10T14:35:59.547665Z","iopub.status.idle":"2024-02-10T14:35:59.903473Z","shell.execute_reply.started":"2024-02-10T14:35:59.547640Z","shell.execute_reply":"2024-02-10T14:35:59.902683Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### Preparing the Dataset and Dataloader :\nThis class is defined to accept the Dataframe as input and generate tokenized output that is used by the Roberta model for training.\n","metadata":{}},{"cell_type":"code","source":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.Phrase\n        self.targets = self.data.Sentiment\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.904577Z","iopub.execute_input":"2024-02-10T14:35:59.904856Z","iopub.status.idle":"2024-02-10T14:35:59.914994Z","shell.execute_reply.started":"2024-02-10T14:35:59.904821Z","shell.execute_reply":"2024-02-10T14:35:59.914107Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"#### Fraction of Data Used for Training and Validation :\n","metadata":{}},{"cell_type":"code","source":"train_size = 0.8\ntrain_data=new_df.sample(frac=train_size,random_state=200)\nval_data=new_df.drop(train_data.index).reset_index(drop=True)\ntrain_data = train_data.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\nprint(\"VALIDATION Dataset: {}\".format(val_data.shape))\n\ntraining_set = SentimentData(train_data, tokenizer, MAX_LEN)\nvalidation_set = SentimentData(val_data, tokenizer, MAX_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.916147Z","iopub.execute_input":"2024-02-10T14:35:59.916475Z","iopub.status.idle":"2024-02-10T14:35:59.955247Z","shell.execute_reply.started":"2024-02-10T14:35:59.916449Z","shell.execute_reply":"2024-02-10T14:35:59.954332Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"FULL Dataset: (156060, 2)\nTRAIN Dataset: (124848, 2)\nTEST Dataset: (31212, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Configuring Training and Validation Parameters with Creation of Corresponding Data Loaders\n","metadata":{}},{"cell_type":"code","source":"\ntrain_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 2\n                }\n\nval_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 2\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\nvalidation_loader = DataLoader(validation_set, **val_params)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.957961Z","iopub.execute_input":"2024-02-10T14:35:59.958262Z","iopub.status.idle":"2024-02-10T14:35:59.983935Z","shell.execute_reply.started":"2024-02-10T14:35:59.958237Z","shell.execute_reply":"2024-02-10T14:35:59.982920Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"#### Creating the Neural Network for Fine Tuning :","metadata":{}},{"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n    \n        self.dropout = torch.nn.Dropout(0.3)\n     \n        self.classifier = torch.nn.Linear(768, 5)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    \n        hidden_state = output_1[0]\n     \n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n       \n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.985130Z","iopub.execute_input":"2024-02-10T14:35:59.985424Z","iopub.status.idle":"2024-02-10T14:35:59.997482Z","shell.execute_reply.started":"2024-02-10T14:35:59.985390Z","shell.execute_reply":"2024-02-10T14:35:59.996665Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model = RobertaClass()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:35:59.998472Z","iopub.execute_input":"2024-02-10T14:35:59.998769Z","iopub.status.idle":"2024-02-10T14:36:00.597591Z","shell.execute_reply.started":"2024-02-10T14:35:59.998744Z","shell.execute_reply":"2024-02-10T14:36:00.596672Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"RobertaClass(\n  (l1): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Loss Function and Optimizer :\n","metadata":{}},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:36:00.598816Z","iopub.execute_input":"2024-02-10T14:36:00.599213Z","iopub.status.idle":"2024-02-10T14:36:00.606741Z","shell.execute_reply.started":"2024-02-10T14:36:00.599178Z","shell.execute_reply":"2024-02-10T14:36:00.606011Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:36:00.608100Z","iopub.execute_input":"2024-02-10T14:36:00.608382Z","iopub.status.idle":"2024-02-10T14:36:00.616186Z","shell.execute_reply.started":"2024-02-10T14:36:00.608358Z","shell.execute_reply":"2024-02-10T14:36:00.615328Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"#### Training Function for RoBERTa Sentiment Analysis Model\nHere we define a training function that trains the model on the training dataset\n","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n   \n    tr_loss = 0\n   \n    n_correct = 0\n    \n    nb_tr_steps = 0\n    nb_tr_examples = 0\n\n    \n    model.train()\n\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n       \n        outputs = model(ids, mask, token_type_ids)\n\n       \n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n\n       \n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calculate_accuracy(big_idx, targets)\n\n     \n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n\n       \n        if _%5000==0:\n            loss_step = tr_loss/nb_tr_steps\n            accu_step = (n_correct*100)/nb_tr_examples\n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n       \n        optimizer.zero_grad()\n        loss.backward()\n      \n        optimizer.step()\n    \n    \n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:36:00.617256Z","iopub.execute_input":"2024-02-10T14:36:00.617586Z","iopub.status.idle":"2024-02-10T14:36:00.627817Z","shell.execute_reply.started":"2024-02-10T14:36:00.617554Z","shell.execute_reply":"2024-02-10T14:36:00.626880Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 4\nfor epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:36:00.629077Z","iopub.execute_input":"2024-02-10T14:36:00.629431Z","iopub.status.idle":"2024-02-10T15:14:54.795752Z","shell.execute_reply.started":"2024-02-10T14:36:00.629400Z","shell.execute_reply":"2024-02-10T15:14:54.794732Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"1it [00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.603122591972351\nTraining Accuracy per 5000 steps: 20.3125\n","output_type":"stream"},{"name":"stderr","text":"1951it [09:42,  3.35it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 0: 65.29059336152761\nTraining Loss Epoch: 0.8403654154418738\nTraining Accuracy Epoch: 65.29059336152761\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  4.92it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7435369491577148\nTraining Accuracy per 5000 steps: 70.3125\n","output_type":"stream"},{"name":"stderr","text":"1951it [09:43,  3.34it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 1: 70.32471485326157\nTraining Loss Epoch: 0.7146328512764417\nTraining Accuracy Epoch: 70.32471485326157\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7087920904159546\nTraining Accuracy per 5000 steps: 71.875\n","output_type":"stream"},{"name":"stderr","text":"1951it [09:43,  3.34it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 2: 72.70040369088812\nTraining Loss Epoch: 0.6553729299793849\nTraining Accuracy Epoch: 72.70040369088812\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  4.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6167944669723511\nTraining Accuracy per 5000 steps: 73.4375\n","output_type":"stream"},{"name":"stderr","text":"1951it [09:43,  3.34it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 3: 74.81657695758042\nTraining Loss Epoch: 0.6079179163256527\nTraining Accuracy Epoch: 74.81657695758042\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Validation Function for RoBERTa Sentiment Analysis Model\nDuring the validation stage we pass the unseen data(Validation Dataset) to the model. This step determines how good the model performs on the unseen data.","metadata":{}},{"cell_type":"code","source":"def valid(model, validation_loader):\n\n  \n    model.eval()\n\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n\n   \n    with torch.no_grad():\n        for _, data in tqdm(enumerate(validation_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n\n           \n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calculate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n\n          \n            if _%5000==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n                \n \n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n\n    return epoch_accu","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:14:54.797871Z","iopub.execute_input":"2024-02-10T15:14:54.798197Z","iopub.status.idle":"2024-02-10T15:14:54.809048Z","shell.execute_reply.started":"2024-02-10T15:14:54.798165Z","shell.execute_reply":"2024-02-10T15:14:54.807909Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"acc = valid(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:14:54.810278Z","iopub.execute_input":"2024-02-10T15:14:54.810564Z","iopub.status.idle":"2024-02-10T15:15:44.284106Z","shell.execute_reply.started":"2024-02-10T15:14:54.810537Z","shell.execute_reply":"2024-02-10T15:15:44.282910Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss per 100 steps: 0.6581960916519165\nValidation Accuracy per 100 steps: 71.875\n","output_type":"stream"},{"name":"stderr","text":"976it [00:49, 19.76it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss Epoch: 0.7687701086406825\nValidation Accuracy Epoch: 69.11444316288608\nAccuracy on test data = 69.11%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}