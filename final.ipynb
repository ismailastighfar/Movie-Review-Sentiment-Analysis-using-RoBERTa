{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10025,"databundleVersionId":32092,"sourceType":"competition"},{"sourceId":6897,"sourceType":"datasetVersion","datasetId":4511}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font  size=\"4\" >Dans ce notebook, nous allons fine-tuning un modèle **RoBERTa** pour résoudre le problème d'**Analyse de Sentiments**.</font>\n","metadata":{}},{"cell_type":"markdown","source":"#### Importation des bibliothèques Python et préparation de l'environnement\n\nÀ cette étape, nous allons importer les bibliothèques et modules nécessaires à l'exécution de notre script. Les bibliothèques sont les suivantes :\n* Pandas\n* PyTorch\n* PyTorch Utils pour Dataset et Dataloader\n* Transformers\n* Modèle et Tokenizer Robert\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\n\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.502712Z","iopub.execute_input":"2024-02-10T14:31:09.503814Z","iopub.status.idle":"2024-02-10T14:31:09.509796Z","shell.execute_reply.started":"2024-02-10T14:31:09.503780Z","shell.execute_reply":"2024-02-10T14:31:09.508761Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"#### Configuration du périphérique pour l'utilisation du GPU:\n","metadata":{}},{"cell_type":"code","source":"\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.511968Z","iopub.execute_input":"2024-02-10T14:31:09.512526Z","iopub.status.idle":"2024-02-10T14:31:09.526148Z","shell.execute_reply.started":"2024-02-10T14:31:09.512486Z","shell.execute_reply":"2024-02-10T14:31:09.525260Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"#### Chargement des données d'entraînement depuis le fichier 'train.tsv':\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', delimiter='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.527238Z","iopub.execute_input":"2024-02-10T14:31:09.527599Z","iopub.status.idle":"2024-02-10T14:31:09.769126Z","shell.execute_reply.started":"2024-02-10T14:31:09.527564Z","shell.execute_reply":"2024-02-10T14:31:09.767907Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.770431Z","iopub.execute_input":"2024-02-10T14:31:09.770794Z","iopub.status.idle":"2024-02-10T14:31:09.777925Z","shell.execute_reply.started":"2024-02-10T14:31:09.770763Z","shell.execute_reply":"2024-02-10T14:31:09.776967Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"(156060, 4)"},"metadata":{}}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.781512Z","iopub.execute_input":"2024-02-10T14:31:09.781873Z","iopub.status.idle":"2024-02-10T14:31:09.794291Z","shell.execute_reply.started":"2024-02-10T14:31:09.781840Z","shell.execute_reply":"2024-02-10T14:31:09.793324Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"   PhraseId  SentenceId                                             Phrase  \\\n0         1           1  A series of escapades demonstrating the adage ...   \n1         2           1  A series of escapades demonstrating the adage ...   \n2         3           1                                           A series   \n3         4           1                                                  A   \n4         5           1                                             series   \n\n   Sentiment  \n0          1  \n1          2  \n2          2  \n3          2  \n4          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['Sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.795339Z","iopub.execute_input":"2024-02-10T14:31:09.795641Z","iopub.status.idle":"2024-02-10T14:31:09.807838Z","shell.execute_reply.started":"2024-02-10T14:31:09.795616Z","shell.execute_reply":"2024-02-10T14:31:09.806890Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"array([1, 2, 3, 4, 0])"},"metadata":{}}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.809047Z","iopub.execute_input":"2024-02-10T14:31:09.809393Z","iopub.status.idle":"2024-02-10T14:31:09.838855Z","shell.execute_reply.started":"2024-02-10T14:31:09.809358Z","shell.execute_reply":"2024-02-10T14:31:09.837834Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"            PhraseId     SentenceId      Sentiment\ncount  156060.000000  156060.000000  156060.000000\nmean    78030.500000    4079.732744       2.063578\nstd     45050.785842    2502.764394       0.893832\nmin         1.000000       1.000000       0.000000\n25%     39015.750000    1861.750000       2.000000\n50%     78030.500000    4017.000000       2.000000\n75%    117045.250000    6244.000000       3.000000\nmax    156060.000000    8544.000000       4.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>78030.500000</td>\n      <td>4079.732744</td>\n      <td>2.063578</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>45050.785842</td>\n      <td>2502.764394</td>\n      <td>0.893832</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>39015.750000</td>\n      <td>1861.750000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>78030.500000</td>\n      <td>4017.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>117045.250000</td>\n      <td>6244.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>156060.000000</td>\n      <td>8544.000000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### On garde les deux colonnes Phrase(attribut) et Sentiment(target)","metadata":{}},{"cell_type":"code","source":"new_df = train[['Phrase', 'Sentiment']]\nnew_df","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.840363Z","iopub.execute_input":"2024-02-10T14:31:09.840779Z","iopub.status.idle":"2024-02-10T14:31:09.856225Z","shell.execute_reply.started":"2024-02-10T14:31:09.840742Z","shell.execute_reply":"2024-02-10T14:31:09.855301Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                                                   Phrase  Sentiment\n0       A series of escapades demonstrating the adage ...          1\n1       A series of escapades demonstrating the adage ...          2\n2                                                A series          2\n3                                                       A          2\n4                                                  series          2\n...                                                   ...        ...\n156055                                          Hearst 's          2\n156056                          forced avuncular chortles          1\n156057                                 avuncular chortles          3\n156058                                          avuncular          2\n156059                                           chortles          2\n\n[156060 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156055</th>\n      <td>Hearst 's</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>156056</th>\n      <td>forced avuncular chortles</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>156057</th>\n      <td>avuncular chortles</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>156058</th>\n      <td>avuncular</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>156059</th>\n      <td>chortles</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>156060 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Définition de certaines variables clés qui seront utilisées plus tard dans l'apprentissage\n","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 52\n\nTRAIN_BATCH_SIZE = 64\n\nVALID_BATCH_SIZE = 64\n\nLEARNING_RATE = 1e-5\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:09.857742Z","iopub.execute_input":"2024-02-10T14:31:09.858043Z","iopub.status.idle":"2024-02-10T14:31:10.102785Z","shell.execute_reply.started":"2024-02-10T14:31:09.858017Z","shell.execute_reply":"2024-02-10T14:31:10.101935Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"#### Prétraitement et Encodage des Données pour l'Analyse de Sentiments avec Utilisation d'un Tokenizerer","metadata":{}},{"cell_type":"code","source":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.Phrase\n        self.targets = self.data.Sentiment\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.104087Z","iopub.execute_input":"2024-02-10T14:31:10.104482Z","iopub.status.idle":"2024-02-10T14:31:10.115608Z","shell.execute_reply.started":"2024-02-10T14:31:10.104447Z","shell.execute_reply":"2024-02-10T14:31:10.114691Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"train_size = 0.8\ntrain_data=new_df.sample(frac=train_size,random_state=200)\ntest_data=new_df.drop(train_data.index).reset_index(drop=True)\ntrain_data = train_data.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\nprint(\"TEST Dataset: {}\".format(test_data.shape))\n\ntraining_set = SentimentData(train_data, tokenizer, MAX_LEN)\ntesting_set = SentimentData(test_data, tokenizer, MAX_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.116928Z","iopub.execute_input":"2024-02-10T14:31:10.117230Z","iopub.status.idle":"2024-02-10T14:31:10.159526Z","shell.execute_reply.started":"2024-02-10T14:31:10.117202Z","shell.execute_reply":"2024-02-10T14:31:10.158579Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"FULL Dataset: (156060, 2)\nTRAIN Dataset: (124848, 2)\nTEST Dataset: (31212, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 4\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 4\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.160643Z","iopub.execute_input":"2024-02-10T14:31:10.160909Z","iopub.status.idle":"2024-02-10T14:31:10.188168Z","shell.execute_reply.started":"2024-02-10T14:31:10.160886Z","shell.execute_reply":"2024-02-10T14:31:10.187182Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Neural Network for Fine Tuning","metadata":{}},{"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n    \n        self.dropout = torch.nn.Dropout(0.6)\n     \n        self.classifier = torch.nn.Linear(768, 5)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    \n        hidden_state = output_1[0]\n     \n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n       \n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.191942Z","iopub.execute_input":"2024-02-10T14:31:10.192312Z","iopub.status.idle":"2024-02-10T14:31:10.200193Z","shell.execute_reply.started":"2024-02-10T14:31:10.192285Z","shell.execute_reply":"2024-02-10T14:31:10.199133Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"model = RobertaClass()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.201601Z","iopub.execute_input":"2024-02-10T14:31:10.201989Z","iopub.status.idle":"2024-02-10T14:31:10.743128Z","shell.execute_reply.started":"2024-02-10T14:31:10.201952Z","shell.execute_reply":"2024-02-10T14:31:10.742204Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"RobertaClass(\n  (l1): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.6, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE,weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.744244Z","iopub.execute_input":"2024-02-10T14:31:10.744531Z","iopub.status.idle":"2024-02-10T14:31:10.752304Z","shell.execute_reply.started":"2024-02-10T14:31:10.744505Z","shell.execute_reply":"2024-02-10T14:31:10.751365Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.753488Z","iopub.execute_input":"2024-02-10T14:31:10.753779Z","iopub.status.idle":"2024-02-10T14:31:10.763501Z","shell.execute_reply.started":"2024-02-10T14:31:10.753754Z","shell.execute_reply":"2024-02-10T14:31:10.762584Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"\n\ndef train(epoch):\n   \n    tr_loss = 0\n   \n    n_correct = 0\n    \n    nb_tr_steps = 0\n    nb_tr_examples = 0\n\n    \n    model.train()\n\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n       \n        outputs = model(ids, mask, token_type_ids)\n\n       \n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n\n       \n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calculate_accuracy(big_idx, targets)\n\n     \n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n\n       \n        if _%100==0:\n            loss_step = tr_loss/nb_tr_steps\n            accu_step = (n_correct*100)/nb_tr_examples\n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n       \n        optimizer.zero_grad()\n        loss.backward()\n      \n        optimizer.step()\n    \n    \n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.764525Z","iopub.execute_input":"2024-02-10T14:31:10.764800Z","iopub.status.idle":"2024-02-10T14:31:10.776044Z","shell.execute_reply.started":"2024-02-10T14:31:10.764774Z","shell.execute_reply":"2024-02-10T14:31:10.775182Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\nfor epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:31:10.777186Z","iopub.execute_input":"2024-02-10T14:31:10.777540Z","iopub.status.idle":"2024-02-10T15:54:29.109241Z","shell.execute_reply.started":"2024-02-10T14:31:10.777506Z","shell.execute_reply":"2024-02-10T15:54:29.108009Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"1it [00:00,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.6298637390136719\nTraining Accuracy per 5000 steps: 23.4375\n","output_type":"stream"},{"name":"stderr","text":"101it [00:52,  1.97it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.2868368908910468\nTraining Accuracy per 5000 steps: 48.62314356435643\n","output_type":"stream"},{"name":"stderr","text":"201it [01:43,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.167832357669944\nTraining Accuracy per 5000 steps: 53.80130597014925\n","output_type":"stream"},{"name":"stderr","text":"301it [02:34,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.0876282926809748\nTraining Accuracy per 5000 steps: 56.566652823920265\n","output_type":"stream"},{"name":"stderr","text":"401it [03:25,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.0389765920187173\nTraining Accuracy per 5000 steps: 58.23332294264339\n","output_type":"stream"},{"name":"stderr","text":"501it [04:17,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.0007399951150555\nTraining Accuracy per 5000 steps: 59.4498502994012\n","output_type":"stream"},{"name":"stderr","text":"601it [05:08,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.9765465546170011\nTraining Accuracy per 5000 steps: 60.37073627287854\n","output_type":"stream"},{"name":"stderr","text":"701it [05:59,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.956782557909907\nTraining Accuracy per 5000 steps: 61.12250356633381\n","output_type":"stream"},{"name":"stderr","text":"801it [06:50,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.941086821862672\nTraining Accuracy per 5000 steps: 61.69241573033708\n","output_type":"stream"},{"name":"stderr","text":"901it [07:41,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.9263125779758415\nTraining Accuracy per 5000 steps: 62.26068257491676\n","output_type":"stream"},{"name":"stderr","text":"1001it [08:33,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.9154113855633464\nTraining Accuracy per 5000 steps: 62.63424075924076\n","output_type":"stream"},{"name":"stderr","text":"1101it [09:24,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.9061644966227699\nTraining Accuracy per 5000 steps: 62.98819255222525\n","output_type":"stream"},{"name":"stderr","text":"1201it [10:15,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8993459863031437\nTraining Accuracy per 5000 steps: 63.23246253122398\n","output_type":"stream"},{"name":"stderr","text":"1301it [11:07,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8910105836382286\nTraining Accuracy per 5000 steps: 63.55327632590315\n","output_type":"stream"},{"name":"stderr","text":"1401it [11:58,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8841527022267137\nTraining Accuracy per 5000 steps: 63.82383119200571\n","output_type":"stream"},{"name":"stderr","text":"1501it [12:49,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8777476565032543\nTraining Accuracy per 5000 steps: 64.04584443704198\n","output_type":"stream"},{"name":"stderr","text":"1601it [13:40,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8720851455681924\nTraining Accuracy per 5000 steps: 64.28306527170518\n","output_type":"stream"},{"name":"stderr","text":"1701it [14:31,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.866837044907345\nTraining Accuracy per 5000 steps: 64.48045267489712\n","output_type":"stream"},{"name":"stderr","text":"1801it [15:23,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.8620072729649774\nTraining Accuracy per 5000 steps: 64.65071488062188\n","output_type":"stream"},{"name":"stderr","text":"1901it [16:14,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.85759244734585\nTraining Accuracy per 5000 steps: 64.79566675433982\n","output_type":"stream"},{"name":"stderr","text":"1951it [16:39,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 0: 64.86127130590799\nTraining Loss Epoch: 0.8562912700238563\nTraining Accuracy Epoch: 64.86127130590799\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  3.36it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5444304943084717\nTraining Accuracy per 5000 steps: 82.8125\n","output_type":"stream"},{"name":"stderr","text":"101it [00:51,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7340449360337588\nTraining Accuracy per 5000 steps: 69.8019801980198\n","output_type":"stream"},{"name":"stderr","text":"201it [01:42,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7276972355830729\nTraining Accuracy per 5000 steps: 70.05597014925372\n","output_type":"stream"},{"name":"stderr","text":"301it [02:34,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7336592106922124\nTraining Accuracy per 5000 steps: 69.77782392026577\n","output_type":"stream"},{"name":"stderr","text":"401it [03:25,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7342151882791162\nTraining Accuracy per 5000 steps: 69.88388403990025\n","output_type":"stream"},{"name":"stderr","text":"501it [04:16,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7334233640791651\nTraining Accuracy per 5000 steps: 70.0505239520958\n","output_type":"stream"},{"name":"stderr","text":"601it [05:07,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7363626167004597\nTraining Accuracy per 5000 steps: 69.82373128119801\n","output_type":"stream"},{"name":"stderr","text":"701it [05:58,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7364530714704375\nTraining Accuracy per 5000 steps: 69.7195970042796\n","output_type":"stream"},{"name":"stderr","text":"801it [06:49,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7347277613093939\nTraining Accuracy per 5000 steps: 69.79946941323345\n","output_type":"stream"},{"name":"stderr","text":"901it [07:41,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.735248582113067\nTraining Accuracy per 5000 steps: 69.76623196448391\n","output_type":"stream"},{"name":"stderr","text":"1001it [08:32,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7353590866485675\nTraining Accuracy per 5000 steps: 69.74431818181819\n","output_type":"stream"},{"name":"stderr","text":"1101it [09:23,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7341220025091578\nTraining Accuracy per 5000 steps: 69.7661217075386\n","output_type":"stream"},{"name":"stderr","text":"1201it [10:14,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7330950070032967\nTraining Accuracy per 5000 steps: 69.78689633638635\n","output_type":"stream"},{"name":"stderr","text":"1301it [11:05,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7316260966404322\nTraining Accuracy per 5000 steps: 69.84771329746349\n","output_type":"stream"},{"name":"stderr","text":"1401it [11:57,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.731888550634813\nTraining Accuracy per 5000 steps: 69.88758029978587\n","output_type":"stream"},{"name":"stderr","text":"1501it [12:48,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.731850402582335\nTraining Accuracy per 5000 steps: 69.88674217188542\n","output_type":"stream"},{"name":"stderr","text":"1601it [13:39,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7315222886411344\nTraining Accuracy per 5000 steps: 69.86941755153029\n","output_type":"stream"},{"name":"stderr","text":"1701it [14:30,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7307836022326555\nTraining Accuracy per 5000 steps: 69.89546590241035\n","output_type":"stream"},{"name":"stderr","text":"1801it [15:21,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7311823732617297\nTraining Accuracy per 5000 steps: 69.87437534702943\n","output_type":"stream"},{"name":"stderr","text":"1901it [16:13,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.7307809382943591\nTraining Accuracy per 5000 steps: 69.87276433456076\n","output_type":"stream"},{"name":"stderr","text":"1951it [16:38,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 1: 69.85534409842369\nTraining Loss Epoch: 0.7311622637134525\nTraining Accuracy Epoch: 69.85534409842369\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  3.19it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6640477180480957\nTraining Accuracy per 5000 steps: 78.125\n","output_type":"stream"},{"name":"stderr","text":"101it [00:51,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6712112293975188\nTraining Accuracy per 5000 steps: 72.54022277227723\n","output_type":"stream"},{"name":"stderr","text":"201it [01:42,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.673568945470734\nTraining Accuracy per 5000 steps: 72.33364427860697\n","output_type":"stream"},{"name":"stderr","text":"301it [02:34,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6733575853398472\nTraining Accuracy per 5000 steps: 72.3733388704319\n","output_type":"stream"},{"name":"stderr","text":"401it [03:25,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6779654064677897\nTraining Accuracy per 5000 steps: 71.94513715710723\n","output_type":"stream"},{"name":"stderr","text":"501it [04:16,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6766197348902088\nTraining Accuracy per 5000 steps: 72.10266966067864\n","output_type":"stream"},{"name":"stderr","text":"601it [05:07,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6755916629476277\nTraining Accuracy per 5000 steps: 72.1245840266223\n","output_type":"stream"},{"name":"stderr","text":"701it [05:58,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6772809609792032\nTraining Accuracy per 5000 steps: 72.13801711840229\n","output_type":"stream"},{"name":"stderr","text":"801it [06:50,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6786301652441609\nTraining Accuracy per 5000 steps: 72.04080836454432\n","output_type":"stream"},{"name":"stderr","text":"901it [07:41,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6796624439537988\nTraining Accuracy per 5000 steps: 71.97038013318534\n","output_type":"stream"},{"name":"stderr","text":"1001it [08:32,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6803007940431456\nTraining Accuracy per 5000 steps: 71.96241258741259\n","output_type":"stream"},{"name":"stderr","text":"1101it [09:23,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6803242857893633\nTraining Accuracy per 5000 steps: 71.95873069936421\n","output_type":"stream"},{"name":"stderr","text":"1201it [10:15,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6801836361595236\nTraining Accuracy per 5000 steps: 71.98688592839301\n","output_type":"stream"},{"name":"stderr","text":"1301it [11:06,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6806237719592271\nTraining Accuracy per 5000 steps: 71.99990392006148\n","output_type":"stream"},{"name":"stderr","text":"1401it [11:57,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6799629973905755\nTraining Accuracy per 5000 steps: 71.96756780870807\n","output_type":"stream"},{"name":"stderr","text":"1501it [12:48,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6809840730076866\nTraining Accuracy per 5000 steps: 71.83960692871419\n","output_type":"stream"},{"name":"stderr","text":"1601it [13:40,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6805050217085225\nTraining Accuracy per 5000 steps: 71.89939881324172\n","output_type":"stream"},{"name":"stderr","text":"1701it [14:31,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6803410641590614\nTraining Accuracy per 5000 steps: 71.89520870076426\n","output_type":"stream"},{"name":"stderr","text":"1801it [15:22,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6808372922634695\nTraining Accuracy per 5000 steps: 71.88627845641311\n","output_type":"stream"},{"name":"stderr","text":"1901it [16:13,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6814858142731379\nTraining Accuracy per 5000 steps: 71.88321935823251\n","output_type":"stream"},{"name":"stderr","text":"1951it [16:39,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 2: 71.88341022683584\nTraining Loss Epoch: 0.6813699304141001\nTraining Accuracy Epoch: 71.88341022683584\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  3.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6676052808761597\nTraining Accuracy per 5000 steps: 76.5625\n","output_type":"stream"},{"name":"stderr","text":"101it [00:51,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6331390157194421\nTraining Accuracy per 5000 steps: 74.4894801980198\n","output_type":"stream"},{"name":"stderr","text":"201it [01:42,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6342958029526383\nTraining Accuracy per 5000 steps: 73.98165422885572\n","output_type":"stream"},{"name":"stderr","text":"301it [02:33,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6391020579591542\nTraining Accuracy per 5000 steps: 73.69705149501661\n","output_type":"stream"},{"name":"stderr","text":"401it [03:25,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6402868790519505\nTraining Accuracy per 5000 steps: 73.55049875311721\n","output_type":"stream"},{"name":"stderr","text":"501it [04:16,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6419295678119697\nTraining Accuracy per 5000 steps: 73.58408183632734\n","output_type":"stream"},{"name":"stderr","text":"601it [05:07,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6427866435189811\nTraining Accuracy per 5000 steps: 73.52069467554077\n","output_type":"stream"},{"name":"stderr","text":"701it [05:59,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6436324912454194\nTraining Accuracy per 5000 steps: 73.37954707560628\n","output_type":"stream"},{"name":"stderr","text":"801it [06:50,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6427811167287767\nTraining Accuracy per 5000 steps: 73.41214107365792\n","output_type":"stream"},{"name":"stderr","text":"901it [07:41,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6419482412401765\nTraining Accuracy per 5000 steps: 73.48258879023308\n","output_type":"stream"},{"name":"stderr","text":"1001it [08:32,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6421446731457343\nTraining Accuracy per 5000 steps: 73.4812062937063\n","output_type":"stream"},{"name":"stderr","text":"1101it [09:24,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6411882577215293\nTraining Accuracy per 5000 steps: 73.46588328792008\n","output_type":"stream"},{"name":"stderr","text":"1201it [10:15,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.641818050093496\nTraining Accuracy per 5000 steps: 73.42839300582848\n","output_type":"stream"},{"name":"stderr","text":"1301it [11:06,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.640792403355825\nTraining Accuracy per 5000 steps: 73.46512298232129\n","output_type":"stream"},{"name":"stderr","text":"1401it [11:57,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6420035192704387\nTraining Accuracy per 5000 steps: 73.43638472519629\n","output_type":"stream"},{"name":"stderr","text":"1501it [12:49,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6423196698489942\nTraining Accuracy per 5000 steps: 73.42292638241173\n","output_type":"stream"},{"name":"stderr","text":"1601it [13:40,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6422328772319994\nTraining Accuracy per 5000 steps: 73.43262023735166\n","output_type":"stream"},{"name":"stderr","text":"1701it [14:31,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6419023807296887\nTraining Accuracy per 5000 steps: 73.43474426807761\n","output_type":"stream"},{"name":"stderr","text":"1801it [15:22,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6425103441269645\nTraining Accuracy per 5000 steps: 73.42275124930595\n","output_type":"stream"},{"name":"stderr","text":"1901it [16:13,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6421869316269133\nTraining Accuracy per 5000 steps: 73.48352840610205\n","output_type":"stream"},{"name":"stderr","text":"1951it [16:39,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 3: 73.4605280020505\nTraining Loss Epoch: 0.6427810407796926\nTraining Accuracy Epoch: 73.4605280020505\n","output_type":"stream"},{"name":"stderr","text":"\n1it [00:00,  3.29it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5708548426628113\nTraining Accuracy per 5000 steps: 68.75\n","output_type":"stream"},{"name":"stderr","text":"101it [00:51,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5989044108013115\nTraining Accuracy per 5000 steps: 75.20111386138613\n","output_type":"stream"},{"name":"stderr","text":"201it [01:42,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6034145321122449\nTraining Accuracy per 5000 steps: 75.20211442786069\n","output_type":"stream"},{"name":"stderr","text":"301it [02:34,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5949993449390133\nTraining Accuracy per 5000 steps: 75.42566445182725\n","output_type":"stream"},{"name":"stderr","text":"401it [03:25,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5953530467061925\nTraining Accuracy per 5000 steps: 75.41692643391521\n","output_type":"stream"},{"name":"stderr","text":"501it [04:16,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5965938066056151\nTraining Accuracy per 5000 steps: 75.45533932135729\n","output_type":"stream"},{"name":"stderr","text":"601it [05:07,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5951212531516635\nTraining Accuracy per 5000 steps: 75.44977121464227\n","output_type":"stream"},{"name":"stderr","text":"701it [05:59,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5977526418492729\nTraining Accuracy per 5000 steps: 75.34325962910128\n","output_type":"stream"},{"name":"stderr","text":"801it [06:50,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5979572203052178\nTraining Accuracy per 5000 steps: 75.32771535580524\n","output_type":"stream"},{"name":"stderr","text":"901it [07:41,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.5986115559025954\nTraining Accuracy per 5000 steps: 75.31215316315205\n","output_type":"stream"},{"name":"stderr","text":"1001it [08:32,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.599758449670199\nTraining Accuracy per 5000 steps: 75.25755494505495\n","output_type":"stream"},{"name":"stderr","text":"1101it [09:24,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6001363415863166\nTraining Accuracy per 5000 steps: 75.21145549500454\n","output_type":"stream"},{"name":"stderr","text":"1201it [10:15,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6014086387486978\nTraining Accuracy per 5000 steps: 75.18604288093256\n","output_type":"stream"},{"name":"stderr","text":"1301it [11:06,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6026719575382763\nTraining Accuracy per 5000 steps: 75.16333589546502\n","output_type":"stream"},{"name":"stderr","text":"1401it [11:57,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6036708700410134\nTraining Accuracy per 5000 steps: 75.11933440399714\n","output_type":"stream"},{"name":"stderr","text":"1501it [12:49,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.604775801330785\nTraining Accuracy per 5000 steps: 75.08744170552964\n","output_type":"stream"},{"name":"stderr","text":"1601it [13:40,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6057736347013529\nTraining Accuracy per 5000 steps: 75.04489381636478\n","output_type":"stream"},{"name":"stderr","text":"1701it [14:31,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6064641799105398\nTraining Accuracy per 5000 steps: 75.02663874191651\n","output_type":"stream"},{"name":"stderr","text":"1801it [15:22,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6062215266957143\nTraining Accuracy per 5000 steps: 75.0381732370905\n","output_type":"stream"},{"name":"stderr","text":"1901it [16:14,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.6073546968341689\nTraining Accuracy per 5000 steps: 74.97123224618622\n","output_type":"stream"},{"name":"stderr","text":"1951it [16:39,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 4: 74.92711136742278\nTraining Loss Epoch: 0.60836047202975\nTraining Accuracy Epoch: 74.92711136742278\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n\n  \n    model.eval()\n\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n\n   \n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n\n           \n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calculate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n\n          \n            if _%500==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n                \n \n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n\n    return epoch_accu","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:29.111278Z","iopub.execute_input":"2024-02-10T15:54:29.111608Z","iopub.status.idle":"2024-02-10T15:54:29.123229Z","shell.execute_reply.started":"2024-02-10T15:54:29.111576Z","shell.execute_reply":"2024-02-10T15:54:29.122253Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"acc = valid(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T15:54:29.124686Z","iopub.execute_input":"2024-02-10T15:54:29.125570Z","iopub.status.idle":"2024-02-10T15:55:47.326271Z","shell.execute_reply.started":"2024-02-10T15:54:29.125530Z","shell.execute_reply":"2024-02-10T15:55:47.325020Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stderr","text":"2it [00:00,  5.34it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss per 100 steps: 0.8402561545372009\nValidation Accuracy per 100 steps: 68.75\n","output_type":"stream"},{"name":"stderr","text":"488it [01:18,  6.25it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss Epoch: 0.7457523661680886\nValidation Accuracy Epoch: 68.38075099320774\nAccuracy on test data = 68.38%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}